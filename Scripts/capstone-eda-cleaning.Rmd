---
title: 'Capstone: EDA & Cleaning'
date: '`Sys.Date()`'
output: html_document
---

```{r notes, eval = FALSE}
### General notes

# ***Profanity***
#   - I think I'll just clip 7 words
#   - https://en.wikipedia.org/wiki/Seven_dirty_words
#   
# ***Libraries?***
#   - `tm`
#   - `quanteda`?
#       - this guy is built on C++
#       - should be fast
```

```{r opts, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

```{r read, eval = FALSE, echo = TRUE}
news   <- readRDS("../samples/news.Rds")
blogs  <- readRDS("../samples/blogs.Rds")
tweets <- readRDS("../samples/tweets.Rds")

all <- c(news, blogs, tweets)
```

```{r all-dfm, eval = FALSE, echo = TRUE}
library(quanteda)

allcorp <- corpus(all)
alldfm <- dfm(
  allcorp,
  ngrams = 1:3
)
```

```{r ngram-eda, echo = TRUE, eval = FALSE}
library(dplyr)
library(stringr)

# create ngram frequencies
freq <- colSums(alldfm)
freq <- data.frame(count = freq) %>% add_rownames(var = "string") %>% tbl_df()
freq <- arrange(freq, desc(count))
```

```{r save, eval = FALSE}
saveRDS(freq, file = "../samples/allfreq.Rds")
```

```{r load}
freq <- readRDS("../samples/allfreq.Rds")
```

```{r flags, echo = TRUE}
# create flags for unigrams, bigrams, & trigrams, etc.
freq <- freq %>% mutate(
  is_unigram = ifelse(str_count(string, "_") == 0, TRUE, FALSE),
  is_bigram  = ifelse(str_count(string, "_") == 1, TRUE, FALSE),
  is_trigram = ifelse(str_count(string, "_") == 2, TRUE, FALSE)
)
```

```{r plots}
library(ggplot2)

freq %>% 
  filter(is_unigram & count > 30000) %>% 
  ggplot(aes(x = reorder(string, -count), y = count)) + 
  geom_bar(stat = "identity", color = "#2ca25f", fill = "#2ca25f") +
  labs(x = "string", y = "count")

freq %>% 
  filter(is_bigram & count > 5000) %>% 
  ggplot(aes(x = reorder(string, -count), y = count)) + 
  geom_bar(stat = "identity", color = "#2ca25f", fill = "#2ca25f") +
  labs(x = "string", y = "count")

freq %>% 
  filter(is_trigram & count > 1000) %>% 
  ggplot(aes(x = reorder(string, -count), y = count)) + 
  geom_bar(stat = "identity", color = "#2ca25f", fill = "#2ca25f") +
  labs(x = "string", y = "count")
```

```{r howmany, eval = FALSE}
### PARKED CODE

HowMany <- function(search, file, pct = TRUE) {
  if (pct) { 
    (length(grep(search, file)) / length(file)) * 100
  } else {
    length(grep(search, file))
  }
}

# abbreviations
HowMany("Dr.", news)
HowMany("Mr.", news)
HowMany("Mrs.", news)

# punct
HowMany("\\.", news)
HowMany("!", news)
HowMany("?", news)
HowMany("-", news)
HowMany(",", news)
```

```{r news-dfm, eval = FALSE}
### PARKED CODE

# leaving in stopwords for now-- they are probably common if the goal is to predict
# text that a user wants to type...

library(quanteda)

newscorp <- corpus(news)
newsdfm  <- dfm(
  newscorp,
  # ignoredFeatures = stopwords("english"),
  ngrams = 1:3
)
```

```{r try-tokenizing, eval = FALSE}
### PARKED CODE

tokens        <- tokenize(tolower(news), removePunct = TRUE)
tokens_nostop <- removeFeatures(tokens, stopwords("english"))
tokens_ngrams <- ngrams(tokens_nostop, n = 1:3)
```

```{r, eval = FALSE}
### PARKED CODE

newscorp   <- corpus(news)

newsdfm <- newscorp %>% 
  toLower() %>% 
  tokenize(removePunct = TRUE, removeDigits = TRUE, removeURL = TRUE,
           ngrams = 1:3) %>% 
  # stop words might be useful, if you're looking at common phrases in english?
  removeFeatures(stopwords("english")) %>% 
  dfm()
```