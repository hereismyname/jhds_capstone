---
title: 'Capstone: EDA & Cleaning'
author: "AM"
date: "December 18, 2015"
output: html_document
---

### General notes

***Profanity***
  - I think I'll just clip 7 words
  - https://en.wikipedia.org/wiki/Seven_dirty_words
  
***Libraries?***
  - `tm`
  - `quanteda`?
      - this guy is built on C++
      - should be fast
  

```{r opts, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

```{r read}
news   <- readRDS("../samples/news.Rds")
blogs  <- readRDS("../samples/blogs.Rds")
tweets <- readRDS("../samples/tweets.Rds")

all <- c(news, blogs, tweets)
```

```{r howmany, eval = FALSE}
HowMany <- function(search, file, pct = TRUE) {
  if (pct) { 
    (length(grep(search, file)) / length(file)) * 100
  } else {
    length(grep(search, file))
  }
}

# abbreviations
HowMany("Dr.", news)
HowMany("Mr.", news)
HowMany("Mrs.", news)

# punct
HowMany("\\.", news)
HowMany("!", news)
HowMany("?", news)
HowMany("-", news)
HowMany(",", news)
```

```{r news-dfm}
# leaving in stopwords for now-- they are probably common if the goal is to predict
# text that a user wants to type...

library(quanteda)

newscorp <- corpus(news)
newsdfm  <- dfm(
  newscorp,
  # ignoredFeatures = stopwords("english"),
  ngrams = 1:3
)
```

```{r all-dfm}
library(quanteda)

allcorp <- corpus(all)
alldfm <- dfm(
  allcorp,
  ngrams = 1:3
)

```


```{r ngram-eda}
library(dplyr)
library(stringr)

# create ngram frequencies
freq <- colSums(alldfm)
freq <- data.frame(count = freq) %>% add_rownames(var = "string") %>% tbl_df()
freq <- arrange(freq, desc(count))
saveRDS(freq, file = "../samples/allfreq.Rds")

# create flags for unigrams, bigrams, & trigrams, etc.
freq <- freq %>% mutate(
  is_unigram = ifelse(str_count(string, "_") == 0, TRUE, FALSE),
  is_bigram  = ifelse(str_count(string, "_") == 1, TRUE, FALSE),
  is_trigram = ifelse(str_count(string, "_") == 2, TRUE, FALSE),
  is_qgram   = ifelse(str_count(string, "_") == 3, TRUE, FALSE)
)
```

```{r plots}
library(ggplot2)

freq %>% 
  filter(is_unigram & count > 30000) %>% 
  ggplot(aes(x = reorder(string, -count), y = count)) + 
  geom_bar(stat = "identity", color = "#2ca25f", fill = "#2ca25f") +
  labs(x = "string", y = "count")

freq %>% 
  filter(is_bigram & count > 5000) %>% 
  ggplot(aes(x = reorder(string, -count), y = count)) + 
  geom_bar(stat = "identity", color = "#2ca25f", fill = "#2ca25f") +
  labs(x = "string", y = "count")

freq %>% 
  filter(is_trigram & count > 1000) %>% 
  ggplot(aes(x = reorder(string, -count), y = count)) + 
  geom_bar(stat = "identity", color = "#2ca25f", fill = "#2ca25f") +
  labs(x = "string", y = "count")
```

### Parked Code

```{r try-tokenizing, eval = FALSE}
tokens        <- tokenize(tolower(news), removePunct = TRUE)
tokens_nostop <- removeFeatures(tokens, stopwords("english"))
tokens_ngrams <- ngrams(tokens_nostop, n = 1:3)
```

```{r, eval = FALSE}
newscorp   <- corpus(news)

newsdfm <- newscorp %>% 
  toLower() %>% 
  tokenize(removePunct = TRUE, removeDigits = TRUE, removeURL = TRUE,
           ngrams = 1:3) %>% 
  # stop words might be useful, if you're looking at common phrases in english?
  removeFeatures(stopwords("english")) %>% 
  dfm()
```

I can pretend that this document-feature-matrix contains a representative sample of all the 2-3grams in English.